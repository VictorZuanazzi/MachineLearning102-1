{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1006c00073df8042ab68279d6c32b96",
     "grade": false,
     "grade_id": "cell-74ae53a2a2f768b8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Lab 1: Independent Component Analysis\n",
    "\n",
    "### Machine Learning 2 (2019)\n",
    "\n",
    "* The lab exercises can be done in groups of two people, or individually.\n",
    "* The deadline is Tuesday September 17th, 16:59.\n",
    "* Assignment should be submitted through Canvas! Make sure to include your and your teammates' names with the submission.\n",
    "* Attach the .IPYNB (IPython Notebook) file containing your code and answers. Naming of the file should be \"studentid1\\_studentid2\\_lab#\", for example, the attached file should be \"12345\\_12346\\_lab1.ipynb\". Only use underscores (\"\\_\") to connect ids, otherwise the files cannot be parsed.\n",
    "\n",
    "Notes on implementation:\n",
    "\n",
    "* You should write your code and answers in an IPython Notebook: http://ipython.org/notebook.html. If you have problems, please ask.\n",
    "* Use __one cell__ for code and markdown answers only!\n",
    "    * Put all code in the cell with the ```# YOUR CODE HERE``` comment and overwrite the ```raise NotImplementedError()``` line.\n",
    "    * For theoretical questions, put your solution using LaTeX style formatting in the YOUR ANSWER HERE cell.\n",
    "* Among the first lines of your notebook should be \"%pylab inline\". This imports all required modules, and your plots will appear inline.\n",
    "* Large parts of you notebook will be graded automatically. Therefore it is important that your notebook can be run completely without errors and within a reasonable time limit. To test your notebook before submission, select Kernel -> Restart \\& Run All."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1e4b25e4b457a2af687799f5941485c",
     "grade": false,
     "grade_id": "cell-2c0707361601df18",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Literature\n",
    "In this assignment, we will implement the Independent Component Analysis algorithm as described in chapter 34 of David MacKay's book \"Information Theory, Inference, and Learning Algorithms\", which is freely available here:\n",
    "http://www.inference.phy.cam.ac.uk/mackay/itila/book.html\n",
    "\n",
    "Read the ICA chapter carefuly before you continue!\n",
    "\n",
    "### Notation\n",
    "\n",
    "$\\mathbf{X}$ is the $M \\times T$ data matrix, containing $M$ measurements at $T$ time steps.\n",
    "\n",
    "$\\mathbf{S}$ is the $S \\times T$ source matrix, containing $S$ source signal values at $T$ time steps. We will assume $S = M$.\n",
    "\n",
    "$\\mathbf{A}$ is the mixing matrix. We have $\\mathbf{X} = \\mathbf{A S}$.\n",
    "\n",
    "$\\mathbf{W}$ is the matrix we aim to learn. It is the inverse of $\\mathbf{A}$, up to indeterminacies (scaling and permutation of sources).\n",
    "\n",
    "$\\phi$ is an elementwise non-linearity or activation function, typically applied to elements of $\\mathbf{W X}$.\n",
    "\n",
    "### Code\n",
    "In the following assignments, you can make use of the signal generators listed below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "09c7db05973b399feae39aada1e549a7",
     "grade": false,
     "grade_id": "cell-3b1901f3dd2b7a59",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed\"\n",
    "\n",
    "# Signal generators\n",
    "def sawtooth(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (((x / period - phase - 0.5) % 1) - 0.5) * 2 * amp\n",
    "\n",
    "def sine_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return np.sin((x / period - phase) * 2 * np.pi) * amp\n",
    "\n",
    "def square_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return ((np.floor(2 * x / period - 2 * phase - 1) % 2 == 0).astype(float) - 0.5) * 2 * amp\n",
    "\n",
    "def triangle_wave(x, period=0.2, amp=1.0, phase=0.):\n",
    "    return (sawtooth(x, period, 1., phase) * square_wave(x, period, 1., phase) + 0.5) * 2 * amp\n",
    "\n",
    "def random_nonsingular_matrix(d=2):\n",
    "    \"\"\"\n",
    "    Generates a random nonsingular (invertible) matrix of shape d*d\n",
    "    \"\"\"\n",
    "    epsilon = 0.1\n",
    "    A = np.random.rand(d, d)\n",
    "    while abs(np.linalg.det(A)) < epsilon:\n",
    "        A = np.random.rand(d, d)\n",
    "    return A\n",
    "\n",
    "def plot_signals(X, title=\"Signals\"):\n",
    "    \"\"\"\n",
    "    Plot the signals contained in the rows of X.\n",
    "    \"\"\"\n",
    "    figure()\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        ax = plt.subplot(X.shape[0], 1, i + 1)\n",
    "        plot(X[i, :])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fbf8317b87200b1d50704a4f9f83327",
     "grade": false,
     "grade_id": "cell-b88ef81e682e8c77",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following code generates some toy data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8c81ee3f333532c3484e99047e1d752",
     "grade": false,
     "grade_id": "cell-cd375ebf3b9d2dc8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "num_sources = 5\n",
    "signal_length = 500\n",
    "t = linspace(0, 1, signal_length)\n",
    "S = np.c_[sawtooth(t), sine_wave(t, 0.3), square_wave(t, 0.4), triangle_wave(t, 0.25), np.random.randn(t.size)].T\n",
    "plot_signals(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35f74a279f9a7f85e4b2c2a3a034a782",
     "grade": false,
     "grade_id": "cell-d3224a04956018cc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.1 Make mixtures (5 points)\n",
    "Write a function `make_mixtures(S, A)' that takes a matrix of source signals $\\mathbf{S}$ and a mixing matrix $\\mathbf{A}$, and generates mixed signals $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd909b0db0bdf3572ce99bb0638cd80e",
     "grade": false,
     "grade_id": "q1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.1 Make mixtures\n",
    "def make_mixtures(S, A):\n",
    "    return np.matmul(A,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "915bfe6e9ca71b5d04d01f9fdc479fd3",
     "grade": true,
     "grade_id": "q1-test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "np.random.seed(42)\n",
    "A = random_nonsingular_matrix(d=S.shape[0])\n",
    "X = make_mixtures(S, A)\n",
    "plot_signals(X, \"Mixtures\")\n",
    "\n",
    "assert X.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7c45d1ca515bba8fa17125fa2423899",
     "grade": false,
     "grade_id": "q2-text",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.2 Histogram (5 points)\n",
    "Write a function `plot_histograms(X)` that takes a data-matrix $\\mathbf{X}$ and plots one histogram for each signal (row) in $\\mathbf{X}$. You can use the `np.histogram()` (followed by `plot`) or `plt.hist()` function. \n",
    "\n",
    "Plot histograms of the sources and the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8d44876a72298fe5657facdaea4aa98",
     "grade": false,
     "grade_id": "q2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.2 Histogram\n",
    "def plot_histograms(X, title=\"Hist\"):\n",
    "    \n",
    "    subs = X.shape[0] if len(X.shape) == 2 else 1\n",
    "    figure(figsize=(20, subs))\n",
    "    \n",
    "    # multiple hists\n",
    "    if len(X.shape) == 2:\n",
    "        for i, row in enumerate(X, start=1):\n",
    "            plt.subplot(1, subs, i)\n",
    "            plt.hist(row)\n",
    "            \n",
    "            if type(title) == list:\n",
    "                plt.title(f\"{title[i-1]} {i-1}\")\n",
    "            else:\n",
    "                plt.title(f\"{title} {i-1}\")\n",
    "    \n",
    "    # one hist\n",
    "    else: \n",
    "        plt.hist(X)\n",
    "        plt.title(f\"{title}\")\n",
    "        \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def make_title(main, additive):\n",
    "    return [main + \"_\" + add for add in additive]\n",
    "\n",
    "signals = [\"sawtooth\", \"sine\", \"square\", \"triangle\", \"normal\"]\n",
    "\n",
    "X_title = make_title(\"X\", signals)\n",
    "plot_histograms(X, title=X_title)\n",
    "\n",
    "S_title = make_title(\"S\", signals)\n",
    "plot_histograms(S, title=S_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c99ca53117a5de6626ac78ec5191e13",
     "grade": false,
     "grade_id": "cell-d14b12856ee3c83c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Which of these distributions (sources or measurements) tend to look more like Gaussians? Can you think of an explanation for this phenomenon? Why is this important for ICA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68363e5a9239df04a2edb134f6fef78f",
     "grade": true,
     "grade_id": "q2_md",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "***\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "***Comparing Source with Mixed distribution:*** We see that mixed distributions (measurements, $X = AS$) exhibit more Gaussian-like histograms. This is because of the Central Limit Theorem that states that when independent random variables are added, the resultant distribution tends to a normal distribution even if the original distributions were not normal. In our case, we mix 5 random distributions (weighted by $A$) and hence we observe similar behavior.\n",
    "\n",
    "***Mixed signals:*** The *random mixed signal* looks like Gaussian distribution the most. This is because $np.random.randn$ is used to generate the signal which internally actually uses the standard normal by default to do so. The *sine* and *triangle* mixed distributions come next and this is because they have similar smooth oscillations about $0$ with very less time at the peak values and most of the time spent in transitions. *Sawtooth* is third because of similar behavior but with drastic transitions. The *square* is last as it has 2 modes in original form and switches between them drastically thus spending most of the time at these peaks.\n",
    "\n",
    "***Source signals:*** The source signals exhibit the least Gaussian like behavior as they are all different distributions and have not been moxed yet for the Central Limit Theorem to apply.\n",
    "\n",
    "This is not suitable for ICA as the ICA algorithm works under the assumption that the source signals ***do not*** have a Gaussian distribution. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implicit priors (20 points)\n",
    "As explained in MacKay's book, an activation function $\\phi$ used in the ICA learning algorithm corresponds to a prior distribution over sources. Specifically, $\\phi(a) = \\frac{d}{da} \\ln p(a)$. For each of the following activation functions, *derive* the source distribution they correspond to.\n",
    "$$\\phi_0(a) = -\\tanh(a)$$\n",
    "$$\\phi_1(a) = -a + \\tanh(a)$$\n",
    "$$\\phi_2(a) = -a^3$$\n",
    "$$\\phi_3(a) = -\\frac{6a}{a^2 + 5}$$\n",
    "\n",
    "Give your answer without the normalizing constant, so an answer of the form $p(a) \\propto \\verb+[answer]+$ is ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e7718327a5140638e6973ae2a5f2c32",
     "grade": true,
     "grade_id": "q3_md",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**ANSWER**\n",
    "\n",
    "***\n",
    "We have,\n",
    "\n",
    "$\\phi(a)=\\frac{d}{d a} \\ln p(a)$ \n",
    "\n",
    "$\\Rightarrow \\int \\phi(a) da=\\ln p(a)$\n",
    "\n",
    "In general, we will also use the following results for the first two part solutions:\n",
    "\n",
    "$tanh(a) = \\frac{\\exp^a - \\exp^{-a}}{\\exp^a + \\exp^{-a}}$\n",
    "$   =  \\frac{1 - \\exp^{-2a}}{1 + \\exp^{-2a}} $\n",
    "\n",
    "Substituting $y = \\exp^{-2a}$\n",
    "\n",
    "$\\Rightarrow a = \\frac{-1}{2} \\log y$\n",
    "$\\Rightarrow \\frac{da}{dy} = \\frac{-1}{2y}$\n",
    "$\\Rightarrow da = \\frac{-1}{2y} dy$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$tanh(a) = \\frac{-1}{2} \\int \\frac{1 - y}{(1 + y)y}dy$\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "**1) $\\ \\ \\  \\phi_0(a)= -tanh(a)$**\n",
    "\n",
    "$\\Rightarrow \\int \\phi(a)da = -\\int tanh(a) da$\n",
    "\n",
    "$ =  \\frac{1}{2} \\int \\frac{1 - y}{(1 + y)y} dy$\n",
    "\n",
    "$ = \\frac{1}{2} \\int \\frac{1 + y - 2y}{(1 + y)y} dy$\n",
    "\n",
    "$ = \\frac{1}{2} \\int \\frac{1 + y}{(1 + y)y}dy - \\frac{1}{2} \\int \\frac{2y}{(1 + y)y}dy$\n",
    "\n",
    "$ = \\frac{1}{2} \\int \\frac{1}{y}dy - \\int \\frac{1}{(1 + y)}dy$\n",
    "\n",
    "$ = \\frac{1}{2} \\ln y - \\ln (1+y) + c$\n",
    "\n",
    "$ = -a - \\ln (1 + \\exp^{-2a}) + c$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$\\ln p(a) = -a - \\ln (1 + \\exp^{-2a}) + c$\n",
    "\n",
    "$p(a) = \\exp{-a - \\ln (1 + \\exp^{-2a}) + c}$\n",
    "\n",
    "$p(a) = \\frac{\\exp^c} {\\exp^a \\cdot (1 + \\exp^{-2a})}$\n",
    "\n",
    "$p(a) \\propto \\frac{1} {\\exp^a \\cdot (1 + \\exp^{-2a})}$\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "**2) $\\ \\ \\  \\phi_1(a)= -a + tanh(a)$**\n",
    "\n",
    "$\\Rightarrow \\int \\phi(a)da = \\int -a da + \\int tanh(a) da$\n",
    "\n",
    "Following similar steps as above, we arrive at:\n",
    "\n",
    "$\\ln p(a) = -\\int a d a- (-a-\\ln (1+e^{-2 a})+c)=-\\frac{1}{2} a^{2}+a+\\ln (1+e^{-2 a})+c$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$p(a) \\propto e^{-\\frac{1}{2} a^{2}+a}+e^{-\\frac{1}{2} a^{2}-a}$\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "**3)$\\ \\ \\  \\phi_2(a)= -a^3$**\n",
    "\n",
    "$\\Rightarrow \\int \\phi(a)da = \\int -a^3 da$\n",
    "\n",
    "$ = \\frac{-1}{4}a^4 + c$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$p(a) \\propto \\exp^{\\frac {-1}{4}a^4}$\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "**4)$\\ \\ \\  \\phi_3(a)= - \\frac{6a}{a^2 + 5}$**\n",
    "\n",
    "$\\Rightarrow \\int \\phi(a)da = \\int-\\frac{6 a}{a^{2}+5} da$\n",
    "\n",
    "$ = -3\\int \\frac{2a}{a^2 + 5}da$\n",
    "\n",
    "$= -3\\ln |a^2+5|+c$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$p(a) \\propto (a^2 + 5)^{-3}$\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c988368caa45cdd67f4849a3600d4aca",
     "grade": false,
     "grade_id": "q3_p0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_0(a):\n",
    "    return -np.tanh(a)\n",
    "\n",
    "def p_0(a):\n",
    "    return 1 / np.cosh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "550925dfeef41c7d86949b463b6449c7",
     "grade": false,
     "grade_id": "q3_p1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_1(a):\n",
    "    return -a + np.tanh(a)\n",
    "\n",
    "def p_1(a):\n",
    "    return np.cosh(a) * np.exp(-(a**2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41afbf475bf8b13a584a9e4559f1b5e0",
     "grade": false,
     "grade_id": "q3_p2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_2(a):\n",
    "    return -a**3\n",
    "\n",
    "def p_2(a):\n",
    "    return np.exp(- (a**4) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1492e648664759bab9cf523d597ac4f4",
     "grade": false,
     "grade_id": "q3_p3",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def phi_3(a):\n",
    "    return -6*a / (a**2 + 5)\n",
    "\n",
    "def p_3(a):\n",
    "    return 1 / (a**2 + 5)**3\n",
    "\n",
    "# # For fun: how does ReLU compares to the other activation fucntions?\n",
    "# # Answer: It sucks! \n",
    "# # Theory of why it does not work\n",
    "\n",
    "# def phi_4(a):\n",
    "#     return np.maximum(a, 0)\n",
    "\n",
    "# def p_4(a):\n",
    "#     return np.maximum(a, 0)**2\n",
    "    \n",
    "# activation_functions = [phi_0, phi_1, phi_2, phi_3, phi_4]\n",
    "# priors = [p_0, p_1, p_2, p_3, p_4]\n",
    "\n",
    "# a = np.linspace(-5, 5, 1000)\n",
    "# for prior in priors:\n",
    "#     assert prior(a).shape == (1000, ), \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1afb0c8d8c70d0e1c8b37ee2b849a049",
     "grade": true,
     "grade_id": "q3_p_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "activation_functions = [phi_0, phi_1, phi_2, phi_3]\n",
    "priors = [p_0, p_1, p_2, p_3]\n",
    "\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "for prior in priors:\n",
    "    assert prior(a).shape == (1000, ), \"Wrong output shape\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32f796b616383f2d260dabae444972bc",
     "grade": false,
     "grade_id": "q3_plot_text",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot the activation functions and the corresponding prior distributions, from $a = -5$ to $5$ (hint: use the lists defined in the cell above). Compare the shape of the priors to the histogram you plotted in the last question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abc7a7000844035a0d733d272264faec",
     "grade": true,
     "grade_id": "q3_plots",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### 1.3 Implicit priors (continued)\n",
    "\n",
    "a = np.linspace(-5, 5, 1000)\n",
    "f, ax = plt.subplots(2, len(priors))\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(20)\n",
    "\n",
    "for i in range(len(priors)): \n",
    "    ax[0, i].plot(a, activation_functions[i](a), label = 'Activation')\n",
    "    ax[1, i].plot(a, priors[i](a), label = 'Prior')\n",
    "    ax[0, i].legend()\n",
    "    ax[1, i].legend()\n",
    "    ax[0, i].set_title(f'Activation phi_{i}')\n",
    "    ax[1, i].set_title(f'Prior p_{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**ANSWER:**\n",
    "\n",
    "We see that all the activation funcitons correspond to priors that more or less look like the standard normal (Gaussian). These prior distributions correspond the most to the ***random*** source signal shape. Amongst the priors, the $p_0$ and $p_3$ look the most like a standrad Normal (due to their bell shape and the tail).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2b90e2314a2a97cba79993eef89c559",
     "grade": false,
     "grade_id": "q4_text",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.4 Whitening (15 points)\n",
    "Some ICA algorithms can only learn from whitened data. Write a method `whiten(X)` that takes a $M \\times T$ data matrix $\\mathbf{X}$ (where $M$ is the dimensionality and $T$ the number of examples) and returns a whitened matrix. If you forgot what whitening is or how to compute it, various good sources are available online, such as http://courses.media.mit.edu/2010fall/mas622j/whiten.pdf. Your function should also center the data before whitening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$cov[X] = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}$, where $\\overline{x}=$ mean of x\n",
    "\n",
    "$\\mathbf{whitened(x)} = \\boldsymbol{\\Lambda}^{-1 / 2} \\boldsymbol{\\Phi}^{T} \\mathbf{x}$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97168afc19ce18ce7d48d4f0b26e93ab",
     "grade": true,
     "grade_id": "q4",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.4 Whitening\n",
    "def whiten(X):\n",
    "    \n",
    "    # Center the data\n",
    "    mean_X = np.mean(X, axis = 1, keepdims=True)\n",
    "    X -= mean_X\n",
    "    \n",
    "    # Normalizes the covariance\n",
    "    lambd, Phi = np.linalg.eig(np.cov(X))\n",
    "    Lambd = np.diag(np.power(lambd, -1/2)) # put eigen values in the main diagonal\n",
    "    A_w = Phi @ Lambd\n",
    "    X_white = A_w.T @ X\n",
    "    \n",
    "    return X_white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79cbb5c6c61c68e2097837ee0f63d456",
     "grade": true,
     "grade_id": "q4_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Test your function\n",
    "Xw = whiten(X)\n",
    "assert Xw.shape == (num_sources, signal_length), \"The shape of your mixed signals is incorrect\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "625949cfd87388ae6dd9a9f18a99c2a5",
     "grade": false,
     "grade_id": "cell-699b10652d80628b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.5 Interpret results of whitening (10 points)\n",
    "Make 3 figures, one for the sources, one for measurements and one for the whitened measurements. In each figure, make $5 \\times 5$ subplots with scatter plots for each pair of signals. Each axis represents a signal and each time-instance is plotted as a dot in this space. You can use the `plt.scatter()` function. Describe what you see.\n",
    "\n",
    "Now compute and visualize the covariance matrix of the sources, the measurements and the whitened measurements. You can visualize each covariance matrix using this code:\n",
    "```python\n",
    "# Dummy covariance matrix C;\n",
    "C = np.eye(5)  \n",
    "ax = imshow(C, cmap='gray', interpolation='nearest')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea944dad9ce3a262c9ff73cbc99042a",
     "grade": true,
     "grade_id": "q5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### 1.5 Interpret results of whitening\n",
    "\n",
    "mixtures = []\n",
    "mix_whiten = []\n",
    "scatter_colors = ['b' , 'r', 'g']\n",
    "\n",
    "for s in signals:\n",
    "    mixtures.append(s+'_mix')\n",
    "    mix_whiten.append(s+'_whitened')\n",
    "\n",
    "\n",
    "def plot_scatter(X, title, subtitles=[], count=0):\n",
    "    \n",
    "    dim = X.shape[0]\n",
    "    f, axes = plt.subplots(dim, dim, figsize=(13, 13))\n",
    "    f.suptitle(title + \"\\n\" + \"-\"*80, y=1.04)\n",
    "    \n",
    "    for row in range(dim):\n",
    "        for col in range(dim):\n",
    "            ax = axes[row,col]\n",
    "            ax.scatter(X[row, :],X[col, :], s= 0.6, c= scatter_colors[count])\n",
    "            ax.set_xlabel(subtitles[row])\n",
    "            ax.set_ylabel(subtitles[col])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return None\n",
    "    \n",
    "    \n",
    "def plot_covariance(X, title, labels=[]):\n",
    "    \n",
    "    f, ax = plt.subplots(1, figsize=(4, 4))\n",
    "    f.suptitle(title + \"\\n\" + \"-\"*80, y=1.04)\n",
    "    \n",
    "    C = np.cov(X) \n",
    "    ax.set_xticks([0,1,2,3,4])\n",
    "    ax.set_xticklabels(labels,rotation = 30, horizontalalignment=\"right\")\n",
    "    ax.set_yticks([0,1,2,3,4])\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax = imshow(C, cmap='gray', interpolation='nearest')  \n",
    "\n",
    "# Plotting Pairwise Scatterplots   \n",
    "plot_scatter(S, 'Pairwise Source Signals Scatterplots', signals,0)\n",
    "plot_scatter(X, 'Pairwise Measurements Scatterplots', mixtures,1)\n",
    "plot_scatter(Xw, 'Pairwise Whitened Measurements Scatterplots', mix_whiten,2)\n",
    "\n",
    "# Plotting Pairwise Covariance\n",
    "plot_covariance(S, \"Pairwise Source Signals Covariance\", signals)\n",
    "plot_covariance(X, \"Pairwise Measurements Covariance\", mixtures)\n",
    "plot_covariance(Xw, \"Pairwise Whitened Measurements Covariance\", mix_whiten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e234e08d4788d7a8c7bc83ae3d6ec5b",
     "grade": false,
     "grade_id": "cell-ced0d6068d7ea315",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Are the signals independent after whitening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c78b3ea401aff2cd6a1c2cf731608f57",
     "grade": true,
     "grade_id": "q5_md",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "***\n",
    "\n",
    "**ANSWER:**\n",
    "\n",
    "The Whitened Measurements are de-correlated, because the covariance is the identity matrix. However we cannot conclude they are independent. In other words, $\\Sigma = I$ is a necessary condition, but it is not sufficient.\n",
    "\n",
    "The scatterplots before show clear linear correlation between measurement pairs. This is also confirmed from the covariance matrix that has patches (and/or shades) of white all over, where white denotes $1$ (highest correlation) and black denotes $0$ (no correlation). \n",
    "\n",
    "After whitening, we can see the scatter plots are all blobs with no apparent rotation visible. Even the covariance matrix has white patches only on the diagonals (obvious high correlation with self) and no correaltion with others (black everywhere).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99a1cdcf7805a6b054096632a8276932",
     "grade": false,
     "grade_id": "cell-04990989c6d56676",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.6 Covariance (5 points)\n",
    "Explain what a covariant algorithm is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4212bce8f348cb27b4fc84801b1964e3",
     "grade": true,
     "grade_id": "q6",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "A Covariant algorithm should give the same result independent of the unit in which the quantities are measured. Notably, the Gradient Descent update $\\Delta W = \\eta \\nabla_W L$ is not consistent, the left side of the equation is of dimensions $[W]$, the the right side is $[W]^{-1}$. Indeed, gradient descent is guaranteed to converge (under some conditions), but only assimptocially. \n",
    "\n",
    "A work around it is to multiply the right side of the equation by a matrix $M$. The Newton-Hapson update does exatly that, it locally aproximates the loss curve with a parabole and uses the hessian to perform the update. For quadratic and noise-free problems, one single update is sufficient to find the minimum. In this case the matrix $M$ encodes the curvature of the loss curve, other update rules can be designed using this curvature.\n",
    "\n",
    "$M$ can be a curvature matrix, but it can also be a metric matrix. In this second case, if there is a metric that defines distances in the parameter space $W$, then $M$ can be obtained. We often use the quadratic metric of the euclidean space, in this case $M = I$. Which is quite convinient because that does not change anyhing on the update rule of gradient descent $\\Delta W = \\eta M \\nabla_W L = \\eta I \\nabla_W L = \\eta \\nabla_W L$. \n",
    "\n",
    "More detailed information can also be found here: http://www.inference.org.uk/mackay/ica.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13e8de49c4a8eb709ff6c9db6f6c1462",
     "grade": false,
     "grade_id": "cell-218d0fc3ca6d2ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.7 Independent Component Analysis (25 points)\n",
    "Implement the covariant ICA algorithm as described in MacKay. Write a function `ICA(X, activation_function, learning_rate)`, that returns the demixing matrix $\\mathbf{W}$. The input `activation_function` should accept a function such as `lambda a: -tanh(a)`. Update the gradient in batch mode, averaging the gradients over the whole dataset for each update. Make it efficient, so use matrix operations instead of loops where possible (loops are slow in interpreted languages such as python and matlab, whereas matrix operations are internally computed using fast C code). Experiment with the learning rate and the initialization of $\\mathbf{W}$. Your algorithm should be able to converge (i.e. `np.linalg.norm(grad) < 1e-5`) in within 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f581345acaf11ff66da37f79f622285c",
     "grade": false,
     "grade_id": "q7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.7 Independent Component Analysis\n",
    "def ICA(X, activation_function, learning_rate=0.25, print_result=True):\n",
    "    \n",
    "    # Initialize W as an orthogonal matrix\n",
    "    # This reduces the number of updates that is necessary.\n",
    "    W, _ = np.linalg.qr(random_nonsingular_matrix(Xw.shape[0]))\n",
    "    \n",
    "    max_iters = 10000\n",
    "    \n",
    "    ## IF LEARNING RATE IS TOO HIGH IT IS OVER WRITEN TO 0.25, AS INSTRUCTED.\n",
    "    if learning_rate >= 1:\n",
    "        learning_rate=0.25     \n",
    "    \n",
    "    for i in range(max_iters):\n",
    "      \n",
    "        # step 1\n",
    "        a = W @ X\n",
    "        \n",
    "        # step 2\n",
    "        z = activation_function(a)\n",
    "\n",
    "        # step 3\n",
    "        x_prime = W.T @ a\n",
    "        \n",
    "        # step 4\n",
    "        grad = W + z @ x_prime.T / x_prime.shape[1] \n",
    "        \n",
    "        # Update W\n",
    "        W += learning_rate * grad\n",
    "        \n",
    "        # Check for the converging criterial\n",
    "        if  np.linalg.norm(grad) < 1e-5:\n",
    "            break\n",
    "    \n",
    "    if print_result:\n",
    "        print(f'Number of iterations: {i +1}, grad: {np.linalg.norm(grad)} ')\n",
    "    \n",
    "    return W\n",
    "\n",
    "lr = .25\n",
    "W_est = ICA(Xw, phi_0, learning_rate=lr)  # Whitened measuement\n",
    "W_est = ICA(X, phi_0, learning_rate=lr)   # raw measuerement\n",
    "W_est = ICA(Xw, phi_1, learning_rate=lr)  \n",
    "W_est = ICA(X, phi_1, learning_rate=lr)\n",
    "W_est = ICA(Xw, phi_2, learning_rate=lr)  \n",
    "W_est = ICA(X, phi_2, learning_rate=lr)   # Does not converge for anyting in the world!\n",
    "# W_est = ICA(Xw, phi_3, learning_rate=lr)  \n",
    "# W_est = ICA(X, phi_3, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a3e233fc032a7d40d2b315756f5b4ab",
     "grade": true,
     "grade_id": "q7_test",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will test your function so make sure it runs with only X and phi as input, and returns only W\n",
    "# Also it should converge for all activation functions\n",
    "\n",
    "W_estimates = [ICA(Xw, activation_function=phi, learning_rate=1.0) for phi in activation_functions]\n",
    "assert all([W_est.shape == (num_sources, num_sources) for W_est in W_estimates])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28123038a523935dbb3f1030ed731cd0",
     "grade": false,
     "grade_id": "cell-c334b668babfc19b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.8 Experiments  (5 points)\n",
    "Run ICA on the provided signals using each activation function $\\phi_0, \\ldots, \\phi_3$ (or reuse `W_estimates`). Use the found demixing matrix $\\mathbf{W}$ to reconstruct the signals and plot the retreived signals for each choice of activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5634a228c22e83f776a6aae263f8c397",
     "grade": true,
     "grade_id": "q8",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.8 Experiments\n",
    "\n",
    "def run_experiments(X_data, activation_functions, lr=0.25, title='', plot=True):\n",
    "\n",
    "    for i, act_func in enumerate(activation_functions):\n",
    "        W = ICA(X_data, act_func, learning_rate=lr, print_result=plot)\n",
    "        if plot:\n",
    "            print(f\"Phi {i}: {title}\")\n",
    "            plot_signals(W @ Xw, f\"Phi {i}: {title}\")\n",
    "    \n",
    "\n",
    "lr =.25\n",
    "# Experiments on white data\n",
    "print(\"################# White Data #################\")\n",
    "run_experiments(Xw, activation_functions, lr=lr, title=\"De-mixed for whitened data\")\n",
    "\n",
    "# Experiments on raw data\n",
    "print(\"################# Raw Data #################\")\n",
    "run_experiments(X, activation_functions, lr=lr, title=\"De-mixed for raw data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc84d60e05b8e0b1fbb556452fb6c998",
     "grade": false,
     "grade_id": "q7_whitening_question",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def does_whitening_make_a_difference():\n",
    "    # Does it make a difference (in terms of speed of convergence) \n",
    "    # if you whiten your data before running ICA?\n",
    "    \n",
    "#     # fact checking\n",
    "#     lr=.25\n",
    "    \n",
    "#     # Time needed for white data\n",
    "#     white_start = time.time()\n",
    "#     run_experiments(Xw, activation_functions, lr=lr, title=\"De-mixed for whitened data\", plot=False)\n",
    "#     white_time = time.time() - white_start\n",
    "    \n",
    "#     # Time needed for raw data\n",
    "#     raw_start = time.time()\n",
    "#     run_experiments(Xw, activation_functions, lr=lr, title=\"De-mixed for raw data\", plot=False)\n",
    "#     raw_time = time.time() - raw_start\n",
    "    \n",
    "#     return white_time < raw_time\n",
    "\n",
    "    return True\n",
    "\n",
    "a = does_whitening_make_a_difference()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b1d872565bfd67647b69995debe886",
     "grade": true,
     "grade_id": "q7_whitening_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(does_whitening_make_a_difference()) == bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a198f770a9935d1a650e4bba2503762f",
     "grade": false,
     "grade_id": "cell-99b165e65b0a60ed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.9 Audio demixing (10 points)\n",
    "The 'cocktail party effect' refers to the ability humans have to attend to one speaker in a noisy room. We will now use ICA to solve a similar but somewhat idealized version of this problem. The code below loads 5 sound files and plots them.\n",
    "\n",
    "Use a random non-singular mixing matrix to mix the 5 sound files. You can listen to the results in your browser using `play_signals`, or save them to disk if this does not work for you. Plot histograms of the mixed audio and use your ICA implementation to de-mix these and reproduce the original source signals. As in the previous exercise, try each of the activation functions.\n",
    "\n",
    "Keep in mind that this problem is easier than the real cocktail party problem, because in real life there are often more sources than measurements (we have only two ears!), and the number of sources is unknown and variable. Also, mixing is not instantaneous in real life, because the sound from one source arrives at each ear at a different point in time. If you have time left, you can think of ways to deal with these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac441f66ac2d1be0b849897cf8678a02",
     "grade": false,
     "grade_id": "cell-0f323b63610fa06e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "# Save mixtures to disk, so you can listen to them in your audio player\n",
    "def save_wav(data, out_file, rate):\n",
    "    scaled = np.int16(data / np.max(np.abs(data)) * 32767)\n",
    "    scipy.io.wavfile.write(out_file, rate, scaled)\n",
    "\n",
    "# Or play them in your browser\n",
    "def play_signals(S, sample_rate, title=\"Signals\"):\n",
    "    display(Markdown(title))\n",
    "    for signal in S:\n",
    "        display(Audio(signal, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6834a2b5abc4349903a28abaff35ab08",
     "grade": false,
     "grade_id": "cell-394dc65db5a26c2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load audio sources\n",
    "source_files = ['beet.wav', 'beet9.wav', 'beet92.wav', 'mike.wav', 'street.wav']\n",
    "wav_data = []\n",
    "sample_rate = None\n",
    "for f in source_files:\n",
    "    sr, data = scipy.io.wavfile.read(f, mmap=False)\n",
    "    if sample_rate is None:\n",
    "        sample_rate = sr\n",
    "    else:\n",
    "        assert(sample_rate == sr)\n",
    "    wav_data.append(data[:190000])  # cut off the last part so that all signals have same length\n",
    "\n",
    "# Create source and measurement data\n",
    "S_audio = np.c_[wav_data]\n",
    "plot_signals(S_audio)\n",
    "play_signals(S_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c05f3f53c93aebda8b45a81d17ed5e01",
     "grade": true,
     "grade_id": "q9",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### 1.9 Audio demixing\n",
    "\n",
    "# Mixing the signals\n",
    "A = random_nonsingular_matrix(d=S_audio.shape[0])\n",
    "X = make_mixtures(S_audio, A)\n",
    "# plot_signals(X, \"Mixtures\")\n",
    "# play_signals(X, sample_rate, \"Mixtures\")\n",
    "\n",
    "# Whitening the signals\n",
    "Xw = whiten(X)\n",
    "# play_signals(Xw, sample_rate, \"Whitened mixtures\")\n",
    "# plot_signals(Xw, \"Whitened mixtures\")\n",
    "Sw = whiten(S_audio.astype(float)) # useful to compare with the de-mixed data\n",
    "\n",
    "# De-mixing signals\n",
    "W_estimates_audio = []\n",
    "lr = 0.1  # fine tunned for the given data.\n",
    "for i, act_func in enumerate(activation_functions):\n",
    "    W = ICA(Xw, act_func, learning_rate=lr)\n",
    "    W_estimates_audio.append(W)\n",
    "    play_signals(W @ Xw , sample_rate, f\"Phi {i} de-mixed\")\n",
    "    plot_signals(W @ Xw, f\"Phi {i} de-mixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e73ba2e2bcaacfb043748d11b67dd3ec",
     "grade": false,
     "grade_id": "q9_report_text",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Report your results. Using which activation functions ICA recovers the sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13100a4d8859d74dcf8e112d3e129b4f",
     "grade": true,
     "grade_id": "q9_report_answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "The sourcse were quite well reconvered by using the activation functions $\\phi_0$ and $\\phi_3$. It is even hard to distinguish between the recovered audio and the original one.\n",
    "\n",
    "The other thwo activation functions did not perform that well. The recovered audios are still quite mixed, so much so it is hard to point the recovered audios to the original ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63f9a427a789d0427bbb67aa7fea2c96",
     "grade": false,
     "grade_id": "cell-c6d32c3d2df970f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 1.10 Excess Kurtosis (15 points)\n",
    "The (excess) kurtosis is a measure of 'peakedness' of a distribution. It is defined as\n",
    "$$\n",
    "\\verb+Kurt+[X] = \\frac{\\mu_4}{\\sigma^4} - 3 = \\frac{\\operatorname{E}[(X-{\\mu})^4]}{(\\operatorname{E}[(X-{\\mu})^2])^2} - 3\n",
    "$$\n",
    "Here, $\\mu_4$ is known as the fourth moment about the mean, and $\\sigma$ is the standard deviation.\n",
    "The '-3' term is introduced so that a Gaussian random variable has 0 excess kurtosis.\n",
    "We will now try to understand the performance of the various activation functions by considering the kurtosis of the corresponding priors, and comparing those to the empirical kurtosis of our data.\n",
    "\n",
    "#### 1.10.1 (10 points)\n",
    "First, compute analytically the kurtosis of the four priors that you derived from the activation functions before. You may find it helpful to use an online service such as [Wolfram Alpha](https://www.wolframalpha.com/) or [Integral Calculator](https://www.integral-calculator.com/) to (help you) evaluate the required integrals. Give your answer as both an exact expression as well as a numerical approximation (for example $\\frac{\\pi}{2} \\approx 1.571$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b228a02df9a733ec2591fcd236f1ee1",
     "grade": true,
     "grade_id": "q10_1",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "All priors are centered in zero, thus $\\mu = 0$. Which makes the calculations easier.\n",
    "\n",
    "First we have to calculate the normalizing constants of each prior:\n",
    "\n",
    "$c_0 = \\int_{-\\infty}^{\\infty} p_0(x) dx = \\int_{-\\infty}^{\\infty} \\frac{1}{cosh(x)} dx = \\pi \\approx 3.14$\n",
    "\n",
    "$c_1 = \\int_{-\\infty}^{\\infty} p_1(x) dx = \\int_{-\\infty}^{\\infty} (cosh(x)exp(-\\frac{x^2}{2})) dx = \\sqrt{2 e \\pi} \\approx  4.13$\n",
    "\n",
    "$c_2 = \\int_{-\\infty}^{\\infty} p_2(x) dx = \\int_{-\\infty}^{\\infty} exp(-\\frac{x^4}{4}) dx = \\frac{\\Gamma(\\frac{1}{4})}{\\sqrt{2}} \\approx 2.56$\n",
    "\n",
    "$c_3 = \\int_{-\\infty}^{\\infty} p_3(x) dx = \\int_{-\\infty}^{\\infty} \\frac{1}{(x^2 + 3)^3} dx = \\frac{\\pi}{24 \\sqrt{3}} \\approx 0.0075$\n",
    "\n",
    "Now we calculate the $\\sigma^4$ of each prior:\n",
    "\n",
    "$\\sigma_0^4 = \\mathcal{E}[(x^2)]^2 =  \\left( \\int_{-\\infty}^{\\infty} p_0(x)x^2 dx \\right)^2  = \\left( \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{x^2}{cosh(x)} dx \\right)^2   = \\frac{\\pi^4}{16} \\approx 6.08$\n",
    "\n",
    "$\\sigma_1^4 = \\mathcal{E}[(x^2)]^2 =  \\left( \\int_{-\\infty}^{\\infty} p_1(x)x^2 dx \\right)^2  = \\left( \\frac{1}{\\sqrt{2 e \\pi}} \\int_{-\\infty}^{\\infty} (cosh(x)exp(-\\frac{x^2}{2}))x^2 dx \\right)^2   = 4$\n",
    "\n",
    "$\\sigma_2^4 = \\mathcal{E}[(x^2)]^2 =  \\left( \\int_{-\\infty}^{\\infty} p_2(x)x^2 dx \\right)^2  = \\left( \\frac{1}{\\frac{\\Gamma(\\frac{1}{4})}{\\sqrt{2}}} \\int_{-\\infty}^{\\infty} exp(-\\frac{x^4}{4})x^2 dx \\right)^2 = 4 \\frac{\\Gamma(\\frac{3}{4})^2}{\\Gamma(\\frac{1}{4})^2} \\approx 0.45$\n",
    "\n",
    "$\\sigma_3^4 = \\mathcal{E}[(x^2)]^2 =  \\left( \\int_{-\\infty}^{\\infty} p_3(x)x^2 dx \\right)^2  = \\left( \\frac{1}{\\frac{\\pi}{24 \\sqrt{3}}} \\int_{-\\infty}^{\\infty} \\frac{x^2}{(x^2 + 3)^3} dx \\right)^2 = 1 $\n",
    "\n",
    "The last piece of the puzzle $\\mu4$ is calculated for each prior:\n",
    "\n",
    "$\\mu4_0 = \\mathcal{E}[(x^4)] =  \\left( \\int_{-\\infty}^{\\infty} p_0(x)x^4 dx \\right)^2  =\\frac{1}{\\pi}  \\int_{-\\infty}^{\\infty} \\frac{x^4}{cosh(x)} dx  = \\frac{5\\pi^4}{16}  \\approx 30.44$\n",
    "\n",
    "$\\mu4_1 = \\mathcal{E}[(x^4)] =  \\left( \\int_{-\\infty}^{\\infty} p_1(x)x^4 dx \\right)^2  = \\frac{1}{\\sqrt{2 e \\pi}} \\int_{-\\infty}^{\\infty} (cosh(x)exp(-\\frac{x^2}{2}))x^4 dx  = 10 $\n",
    "\n",
    "$\\mu4_2 = \\mathcal{E}[(x^4)] =  \\left( \\int_{-\\infty}^{\\infty} p_2(x)x^4 dx \\right)^2  = \\frac{1}{\\frac{\\Gamma(\\frac{1}{4})}{\\sqrt{2}}} \\int_{-\\infty}^{\\infty} exp(-\\frac{x^4}{4})x^4 dx  = 1$\n",
    "\n",
    "$\\mu4_3 = \\mathcal{E}[(x^4)] =  \\left( \\int_{-\\infty}^{\\infty} p_3(x)x^4 dx \\right)^2  = \\frac{1}{\\frac{\\pi}{24 \\sqrt{3}}} \\int_{-\\infty}^{\\infty}  \\frac{x^4}{(x^2 + 3)^3} dx  = 9$\n",
    "\n",
    "Finally, we can calculate the Kurtosis:\n",
    "\n",
    "$K_0 = \\frac{\\mu4_0}{\\sigma_0^4 } - 3 =  \\frac{\\frac{5\\pi^4}{16}}{\\frac{\\pi^4}{16}} - 3 = 2 $\n",
    "\n",
    "$K_1 = \\frac{\\mu4_1}{\\sigma_1^4 } - 3 =  \\frac{10}{4} - 3 = -0.5$\n",
    "\n",
    "$K_2 = \\frac{\\mu4_2}{\\sigma_2^4 } - 3 =  \\frac{1}{4 \\frac{\\Gamma(\\frac{3}{4})^2}{\\Gamma(\\frac{1}{4})^2}} - 3 \\approx -0.81$\n",
    "\n",
    "$K_3 = \\frac{\\mu4_3}{\\sigma_3^4 } - 3 =  \\frac{9}{1} - 3 =6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6e3f3dc5475c4e1f9166baad21f50b0",
     "grade": false,
     "grade_id": "q10_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Include your answer here (you can use math.gamma if needed)\n",
    "def get_kurtosis():\n",
    "    # Return a list with 4 numbers / expressions\n",
    "    \n",
    "    Ks = [2, -.5, np.power(math.gamma(1/4), 2)/(np.power(math.gamma(3/4), 2) * 4) - 3, 6]\n",
    "    return Ks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "470ef529b6f5c2b63354dc6e2dcfa12c",
     "grade": true,
     "grade_id": "q10_test",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check\n",
    "kurtosis = get_kurtosis()\n",
    "print (kurtosis)\n",
    "assert len(kurtosis) == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81adb9331459b7ad5b4715db50bd818c",
     "grade": false,
     "grade_id": "cell-dfc3f096ad8ab2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 1.10.2 (5 points)\n",
    "Now use the `scipy.stats.kurtosis` function, with the `fisher` option set to `True`, to compute the empirical kurtosis of the dummy signals and the real audio signals.\n",
    "\n",
    "Can you use this data to explain the performance of the various activation functions on the synthetic and real data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb69b34260231dc721c05556aae55f4",
     "grade": false,
     "grade_id": "q10_2_code",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### 1.10.2 Excess Kurtosis\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "kurts_dummy = kurtosis(S, fisher=True, axis=1)\n",
    "kurts_audio = kurtosis(S_audio, fisher=True, axis=1)\n",
    "    \n",
    "print(f\"Dummy: {kurts_dummy}\")\n",
    "print(f\"Audio: {kurts_audio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38d2dc1d593a547e0fab40695e595e68",
     "grade": true,
     "grade_id": "q10_2_md",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
